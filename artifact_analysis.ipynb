{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.transforms import Bbox\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.colors as mcolors\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from zoneinfo import ZoneInfo\n",
    "from datetime import timedelta, datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import stats\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import utils\n",
    "import json_file_utils\n",
    "import stats_utils\n",
    "import artifact_correct\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in patient info and translate it into a dictionary\n",
    "with open('patient_info.yaml') as stream:\n",
    "    patient_info = yaml.safe_load(stream)\n",
    "patient_dict = {d['id']: d for d in patient_info['patients']}\n",
    "\n",
    "# Get list of patient IDs\n",
    "ids = list(patient_dict.keys())\n",
    "\n",
    "# Set up some other utility stuff\n",
    "central_time = ZoneInfo('America/Chicago')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in raw data from JSONs on Elias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_dfs = []\n",
    "pt_raw_dfs = []\n",
    "for pt_id in ids:\n",
    "    # Get path to JSON files containing data for the current patient.\n",
    "    pt_percept_dir = Path(patient_dict[pt_id]['data_path'])\n",
    "    jsons = json_file_utils.get_json_filenames(pt_percept_dir)\n",
    "    \n",
    "    raw_data_list = []\n",
    "    for filename in tqdm(jsons):\n",
    "        # Read LFP and stim data from JSON file.\n",
    "        try:\n",
    "            raw = json_file_utils.chronic_lfp_from_json(filename)\n",
    "        except (PermissionError, json.JSONDecodeError) as e:\n",
    "            continue\n",
    "    \n",
    "        # Extract raw stim and LFP data from JSON to dataframe.\n",
    "        if not raw.empty:\n",
    "            raw_data_list.append(raw)\n",
    "\n",
    "    # Concatenate individual files' data into a single dataframe per patient\n",
    "    try:\n",
    "        pt_raw_df = pd.concat(raw_data_list, ignore_index=True)\n",
    "        pt_raw_df['pt_id'] = pt_id\n",
    "        if pt_raw_df.size != 0:\n",
    "            pt_raw_dfs.append(pt_raw_df)\n",
    "    except ValueError as e:\n",
    "        print(f'no chronic LFP power data from {pt_id}')\n",
    "\n",
    "raw_df = pd.concat(pt_raw_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VC/VS Only**\n",
    "\n",
    "Most of our patients only have bilateral VC/VS implants. Their leads are connected down to a single IPG on the right side of their chest. The device measures from both leads in parallel, and an average power value from each hemisphere is calculated every 10 minutes, synced across hemispheres.\n",
    "\n",
    "| Actual lead location | Labeled lead location in JSON | IPG side | Requires correction |\n",
    "| --- | --- | --- | --- |\n",
    "| Left VC/VS | Left VC/VS | Right chest | No |\n",
    "| Right VC/VS | Right VC/VS | Right chest | No |\n",
    "\n",
    "Note that the lead location is listed as \"Other\" instead of \"VC/VS\" for some patients.\n",
    "\n",
    "**VC/VS + GPI**\n",
    "\n",
    "Some OCD patients have comorbid Tourette syndrome (B001, B005, B008, B010), and in addition to their bilateral VC/VS leads receive two additional leads implanted bilaterally in the GPI. These patients then receive two IPGs, one in each half of the chest, and two leads connect to each IPG. The IPG in the right side of the chest is connected to both VC/VS leads, and the left IPG is likewise connected to the GPI leads. No correction is required for these patients, and the files containing GPI data are simply removed for our analysis here.\n",
    "\n",
    "| Actual lead location | Labeled lead location in JSON | IPG side | Requires correction |\n",
    "| --- | --- | --- | --- |\n",
    "| Left VC/VS | Left VC/VS | Right chest | No |\n",
    "| Right VC/VS | Right VC/VS | Right chest | No |\n",
    "| Left GPI | Left GPI | Left chest | No |\n",
    "| Right GPI | Right GPI | Left chest | No |\n",
    "\n",
    "**VC/VS + OFC (B014 and B015)**\n",
    "\n",
    "A handful of patients (B014, B015, and B017) received leads implanted bilaterally in the VC/VS and bilaterally in the OFC. These patients also receive two IPGs in the chest with the two right hemisphere leads connected to the right IPG and the two left hemisphere leads connected to the left IPG. The Medtronic Percept device collects up to two streams of data, one from each from its two connected leads, and it assigns one lead a hemisphere label of “right” and the other a label of “left.” Each lead may have its own implant location selected from a list of presets. For these patients, because both leads feeding into each device come from the same hemisphere, we must correct the data to properly determine where it came from. In B014 and B015, the left VC/VS lead is connected to the left IPG and given a label of \"left VC/VS,\" and the left OFC lead is connected to the left IPG and given a label of \"right Other\" (OFC is not one of the preset lead location options). Likewise, the right VC/VS lead and right OFC lead are both fed to the right IPG with labels of \"right VC/VS\" and \"left Other,\" respectively.\n",
    "\n",
    "| Actual lead location | Labeled lead location in JSON | IPG side | Requires correction |\n",
    "| --- | --- | --- | --- |\n",
    "| Left VC/VS | Left VC/VS | Left chest | No |\n",
    "| Right VC/VS | Right VC/VS | Right chest | No |\n",
    "| Left OFC | Right Other | Left chest | Yes |\n",
    "| Right OFC | Left Other | Right chest | Yes |\n",
    "\n",
    "Note that B016 and B018 also have VC/VS and OFC implants, but we do not have any chronic LFP data from their devices.\n",
    "\n",
    "**VC/VS + OFC (B017)**\n",
    "\n",
    "This patient's system is configured slightly differently from the other VC/VS+OFC patients. The right IPG's labels are swapped, so the right VC/VS lead is labeled in the device's data collected as \"left VC/VS,\" and the right OFC lead is labeled as \"right Other.\" This results in both VC/VS leads being labeled as left VC/VS and both OFC leads being labeled as right Other.\n",
    "\n",
    "| Actual lead location | Labeled lead location in JSON | IPG side | Requires correction |\n",
    "| --- | --- | --- | --- |\n",
    "| Left VC/VS | Left VC/VS | Left chest | No |\n",
    "| Right VC/VS | Left VC/VS | Right chest | Yes |\n",
    "| Left OFC | Right Other | Left chest | Yes |\n",
    "| Right OFC | Right Other | Right chest | No |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.query('pt_id != \"B017\" or source_file != \"Report_Json_Session_Report_017_01R_20240516T152238.json\"', inplace=True) # This file is for the wrong patient.\n",
    "\n",
    "# B017's data is labeled differently from other OFC patients --> correct it to match the others.\n",
    "right_chest_jsons_017 = list((Path(patient_dict['B017']['data_path']) / 'R').glob('[!.]*.json'))\n",
    "right_chest_jsons_017 = [f.name for f in right_chest_jsons_017]\n",
    "right_chest_data_017 = raw_df.query('pt_id == \"B017\" and source_file in @right_chest_jsons_017').copy()\n",
    "right_chest_data_017[['lfp_left', 'stim_left', 'lfp_right', 'stim_right', 'left_lead_location', 'right_lead_location']] = right_chest_data_017[['lfp_right', 'stim_right', 'lfp_left', 'stim_left', 'right_lead_location', 'left_lead_location']].copy()\n",
    "raw_df.drop(labels=right_chest_data_017.index, inplace=True)\n",
    "raw_df = pd.concat([raw_df, right_chest_data_017], ignore_index=True)\n",
    "\n",
    "# Correct mislabeled data for OFC patients\n",
    "ofc_patients = ['B014', 'B015', 'B016', 'B017', 'B018']\n",
    "ofc_pt_right_ipg_data_vcvs = raw_df.query('pt_id in @ofc_patients and left_lead_location == \"OTHER\" and right_lead_location == \"VC/VS\"').copy()\n",
    "ofc_pt_right_ipg_data_ofc = ofc_pt_right_ipg_data_vcvs.copy()\n",
    "ofc_pt_right_ipg_data_vcvs[['lfp_left', 'stim_left']] = np.nan\n",
    "ofc_pt_right_ipg_data_vcvs[['right_lead_location', 'left_lead_location']] = 'VC/VS'\n",
    "ofc_pt_right_ipg_data_ofc[['lfp_right', 'stim_right']] = ofc_pt_right_ipg_data_ofc[['lfp_left', 'stim_left']].copy()\n",
    "ofc_pt_right_ipg_data_ofc[['lfp_left', 'stim_left']] = np.nan\n",
    "ofc_pt_right_ipg_data_ofc[['right_lead_location', 'left_lead_location']] = 'OFC'\n",
    "\n",
    "ofc_pt_left_ipg_data_vcvs = raw_df.query('pt_id in @ofc_patients and left_lead_location == \"VC/VS\" and right_lead_location == \"OTHER\"').copy()\n",
    "ofc_pt_left_ipg_data_ofc = ofc_pt_left_ipg_data_vcvs.copy()\n",
    "ofc_pt_left_ipg_data_vcvs[['lfp_right', 'stim_right']] = np.nan\n",
    "ofc_pt_left_ipg_data_vcvs[['right_lead_location', 'left_lead_location']] = 'VC/VS'\n",
    "ofc_pt_left_ipg_data_ofc[['lfp_left', 'stim_left']] = ofc_pt_left_ipg_data_ofc[['lfp_right', 'stim_right']].copy()\n",
    "ofc_pt_right_ipg_data_ofc[['lfp_right', 'stim_right']] = np.nan\n",
    "ofc_pt_right_ipg_data_ofc[['right_lead_location', 'left_lead_location']] = 'OFC'\n",
    "\n",
    "# Finish up the OFC data\n",
    "raw_df.drop(labels=ofc_pt_right_ipg_data_vcvs.index, inplace=True)\n",
    "raw_df.drop(labels=ofc_pt_left_ipg_data_vcvs.index, inplace=True)\n",
    "raw_df = pd.concat([raw_df, ofc_pt_right_ipg_data_vcvs, ofc_pt_left_ipg_data_vcvs, ofc_pt_right_ipg_data_ofc, ofc_pt_left_ipg_data_ofc], ignore_index=True)\n",
    "raw_df.dropna(subset=['lfp_left', 'lfp_right'], how='all', inplace=True, ignore_index=True)\n",
    "raw_df.sort_values(by=['pt_id', 'left_lead_location', 'timestamp'], inplace=True, ignore_index=True)\n",
    "\n",
    "# Relabel all remaining \"OTHER\" lead locations to VC/VS\n",
    "raw_df.loc[raw_df['left_lead_location'] == \"OTHER\", 'left_lead_location'] = 'VC/VS'\n",
    "raw_df.loc[raw_df['right_lead_location'] == \"OTHER\", 'right_lead_location'] = 'VC/VS'\n",
    "\n",
    "assert (raw_df['left_lead_location'] == raw_df['right_lead_location']).all()\n",
    "raw_df['lead_location'] = raw_df['left_lead_location'].where(raw_df['left_lead_location'] == raw_df['right_lead_location'], None)\n",
    "raw_df.drop(columns=['left_lead_location', 'right_lead_location'], inplace=True)\n",
    "\n",
    "assert (raw_df['left_lead_model'] == raw_df['right_lead_model']).all()\n",
    "raw_df['lead_model'] = raw_df['left_lead_model'].where(raw_df['left_lead_model'] == raw_df['right_lead_model'], None)\n",
    "raw_df.drop(columns=['left_lead_model', 'right_lead_model'], inplace=True)\n",
    "\n",
    "# Get rid of all non-VC/VS data\n",
    "raw_df.query('lead_location == \"VC/VS\"', inplace=True)\n",
    "\n",
    "# Sort and format the raw data\n",
    "raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'])\n",
    "raw_df.sort_values(by=['pt_id', 'lead_location', 'timestamp'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When new data is measured, the Percept device overwrites the oldest data, always storing exactly $N$ days worth of data, where $N$ is 35 or 60, depending on the model of the Percept device. This means that data files always contain the most recent $N$ days of data, not just the data collected since the last download. Thus, a single data sample or a collection of many consecutive samples may be duplicated several times throughout the dataset due to it appearing in multiple data files.\n",
    "\n",
    "At clinical visits, we sync the device with network time, and it applies a transformation to the collected data in an attempt to make its timestamps more accurate. This means that the duplicated data streams may have slightly offset timestamps. To severely mitigate this issue, we only keep data from the most recent file for any particular timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any duplicate data points included in multiple files (sometimes even with time drift)\n",
    "# Step 1: Get first timestamp per pt_id and source_file\n",
    "file_start_times = (\n",
    "    raw_df.groupby(['pt_id', 'source_file'])['timestamp']\n",
    "    .min()\n",
    "    .reset_index()\n",
    "    .rename(columns={'timestamp': 'file_start_time'})\n",
    ")\n",
    "file_end_times = (\n",
    "    raw_df.groupby(['pt_id', 'source_file'])['timestamp']\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={'timestamp': 'file_end_time'})\n",
    ")\n",
    "\n",
    "# Step 2: Sort files in temporal order per pt_id\n",
    "file_order = (\n",
    "    file_start_times.sort_values(['pt_id', 'file_start_time'])\n",
    "    .groupby('pt_id')['source_file']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Step 3: For each file, record the not-yet-seen files (per pt_id)\n",
    "file_to_future = {\n",
    "    (pt_id, current_file): set(files[i+1:])\n",
    "    for pt_id, files in file_order.items()\n",
    "    for i, current_file in enumerate(files)\n",
    "}\n",
    "\n",
    "# Get start and end times for each future file\n",
    "file_bounds = file_start_times.merge(file_end_times, on=['pt_id', 'source_file']).rename(columns={'source_file': 'future_file'})\n",
    "expanded = []\n",
    "for (pt_id, source_file), future_files in file_to_future.items():\n",
    "    for future_file in future_files:\n",
    "        expanded.append({'pt_id': pt_id, 'source_file': source_file, 'future_file': future_file})\n",
    "future_map = pd.DataFrame(expanded)\n",
    "\n",
    "future_map = future_map.merge(file_bounds, on=['pt_id', 'future_file'], how='left')\n",
    "future_map = future_map.groupby(['pt_id', 'source_file']).apply(lambda x: list(zip(x['file_start_time'], x['file_end_time'])), include_groups=False).reset_index(name='time_intervals')\n",
    "\n",
    "raw_df = raw_df.merge(future_map, on=['pt_id', 'source_file'], how='left')\n",
    "remove = raw_df.apply(lambda row: isinstance(row['time_intervals'], list) and any(start <= row['timestamp'] <= end for start, end in row['time_intervals']), axis=1)\n",
    "raw_df = raw_df[~remove]\n",
    "raw_df.drop(columns=['time_intervals'], inplace=True)\n",
    "raw_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process raw data\n",
    "\n",
    "# Fill outliers: define which filling method(s) you want to use\n",
    "outlier_fill_methods = {\n",
    "    'threshold': artifact_correct.threshold_outliers,\n",
    "    'OvER': artifact_correct.fill_outliers_OvER\n",
    "}\n",
    "\n",
    "raw_df.rename(columns={'lfp_left': 'lfp_left_raw', 'lfp_right': 'lfp_right_raw'}, inplace=True)\n",
    "\n",
    "# Put timestamps in datetime format and drop any remaining duplicate readings\n",
    "raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'])\n",
    "raw_df.drop_duplicates(subset=['pt_id', 'timestamp', 'lead_location'], inplace=True)\n",
    "raw_df.sort_values(['pt_id', 'lead_location', 'timestamp'], inplace=True, ignore_index=True)\n",
    "\n",
    "# Convert timestamp column into central time for analysis.\n",
    "raw_df['CT_timestamp'] = raw_df['timestamp'].dt.tz_convert(central_time)\n",
    "\n",
    "# Add columns for 10 minute bin reading falls into (round up to 10 minute interval)\n",
    "raw_df['time_bin'] = raw_df['timestamp'].dt.ceil('10min')\n",
    "raw_df['time_bin_time'] = raw_df['time_bin'].dt.time\n",
    "\n",
    "# If there are still any duplicates, eliminate them.\n",
    "processed_df = raw_df.drop_duplicates(subset=['pt_id', 'time_bin', 'lead_location', 'lfp_left_raw', 'lfp_right_raw'], keep='first', ignore_index=True).copy()\n",
    "\n",
    "# Add column for days since first VC/VS DBS activation\n",
    "dbs_on_date_dict = {}\n",
    "for pt_id in processed_df['pt_id'].unique():\n",
    "    dbs_on_date_dict[pt_id] = utils.get_dbs_on_date(patient_dict[pt_id]['dbs_on_date'])\n",
    "processed_df['dbs_start_date'] = processed_df['pt_id'].map(dbs_on_date_dict)\n",
    "processed_df['days_since_dbs'] = (processed_df['CT_timestamp'].dt.date - processed_df['dbs_start_date']).apply(lambda td: td.days)\n",
    "\n",
    "# Add empty new rows to fill in missing timestamps, and corrected outliers and interpolate missing rows.\n",
    "added_rows = processed_df.groupby(['pt_id', 'lead_location'], group_keys=False).apply(lambda g: utils.add_empty_rows(g, g.name[0], g.name[1], dbs_on_date=dbs_on_date_dict[g.name[0]]), include_groups=False)\n",
    "if not added_rows.empty:\n",
    "    processed_df = pd.concat((processed_df, added_rows), ignore_index=True)\n",
    "processed_df.sort_values(by=['pt_id', 'lead_location', 'timestamp'], inplace=True, ignore_index=True)\n",
    "\n",
    "# Fix overvoltages and fill in holes in data using the specified method(s).\n",
    "for name, func in outlier_fill_methods.items():\n",
    "    outlier_corrected_cols = processed_df.groupby(['pt_id', 'lead_location'], group_keys=False)\\\n",
    "        .apply(lambda g: func(g, cols_to_fill=['lfp_left_raw', 'lfp_right_raw']), include_groups=False)\n",
    "    processed_df = pd.merge(processed_df, outlier_corrected_cols, how='outer', left_index=True, right_index=True)\n",
    "    for hem, other_hem in ['left', 'right'], ['right', 'left']:\n",
    "        # Get rid of all rows where other hem is not NaN but this hem is NaN (these were likely never meant to have data from this hem).\n",
    "        interp_df = processed_df.loc[processed_df[f'lfp_{hem}_raw'].notna() | processed_df[f'lfp_{other_hem}_raw'].isna()]\n",
    "        filled_cols = interp_df.groupby(['pt_id', 'lead_location'], group_keys=False)\\\n",
    "            .apply(lambda g: artifact_correct.interpolate_holes(g, cols_to_fill=[col for col in outlier_corrected_cols.columns if ((hem in col) and ('num_overages' not in col))]), include_groups=False)\n",
    "        processed_df = pd.merge(processed_df, filled_cols, how='outer', left_index=True, right_index=True)\n",
    "corr_col_names = [f'lfp_left_{name}_interpolate' for name in outlier_fill_methods.keys()] + \\\n",
    "                 [f'lfp_right_{name}_interpolate' for name in outlier_fill_methods.keys()]\n",
    "processed_df.dropna(subset=corr_col_names, how='all', inplace=True) # Get rid of any rows that are still empty in both hems\n",
    "\n",
    "# Mark rows that were changed with 'corrected' tag.\n",
    "processed_df['left_corrected'] = False\n",
    "processed_df['right_corrected'] = False\n",
    "for name in outlier_fill_methods.keys():\n",
    "    processed_df.loc[processed_df[f'lfp_left_{name}_interpolate'].notna() & (processed_df['lfp_left_raw'] != processed_df[f'lfp_left_{name}_interpolate']), 'left_corrected'] = True\n",
    "    processed_df.loc[processed_df[f'lfp_right_{name}_interpolate'].notna() & (processed_df['lfp_right_raw'] != processed_df[f'lfp_right_{name}_interpolate']), 'right_corrected'] = True\n",
    "\n",
    "# Z score LFP data within each day.\n",
    "processed_df = processed_df.reset_index(drop=True)\n",
    "groups = processed_df.groupby(['pt_id', 'lead_location', 'days_since_dbs'], group_keys=False)\n",
    "zscored_data = groups.apply(lambda g: utils.zscore_group(g, cols_to_zscore=corr_col_names), include_groups=False)\n",
    "zscored_cols = zscored_data.columns\n",
    "processed_df = pd.merge(processed_df, zscored_data, how='outer', left_index=True, right_index=True) # There must be a better way to do this\n",
    "\n",
    "df = processed_df.copy()\n",
    "\n",
    "# Drop any remaining duplicate readings and sort DF.\n",
    "df.drop_duplicates(['pt_id', 'lfp_left_raw', 'lfp_right_raw', 'stim_left', 'stim_right', 'lead_location', 'time_bin'], inplace=True, ignore_index=True)\n",
    "df.sort_values(by=['pt_id', 'lead_location', 'timestamp'], inplace=True, ignore_index=True)\n",
    "\n",
    "# Get rid of any uninterpretable data\n",
    "df.query('pt_id != \"B001\" or days_since_dbs <= 100', inplace=True) # Cut off B001 after 100 days (opted out of research)\n",
    "df.query('pt_id != \"B015\" or days_since_dbs >= 830', inplace=True) # Cut out B015's data before 830 days (almost 100% outliers)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Mark outliers.\n",
    "df['is_outlier_left'] = (df['lfp_left_raw'] >= ((2 ** 32) - 1) / 60) & (df['lfp_left_raw'].notna())\n",
    "df['is_outlier_right'] = (df['lfp_right_raw'] >= ((2 ** 32) - 1) / 60) & (df['lfp_right_raw'].notna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per = pd.DataFrame(df['pt_id'].value_counts(sort=False))\n",
    "samples_per['unique days'] = df.groupby('pt_id')['days_since_dbs'].nunique()\n",
    "samples_per['days pre-DBS'] = df.query('days_since_dbs < 0').groupby('pt_id')['days_since_dbs'].nunique()\n",
    "samples_per['days pre-DBS'] = samples_per['days pre-DBS'].fillna(0).astype(int)\n",
    "samples_per['days post-DBS'] = df.query('days_since_dbs > 0').groupby('pt_id')['days_since_dbs'].nunique()\n",
    "samples_per['days post-DBS'] = samples_per['days post-DBS'].fillna(0).astype(int)\n",
    "samples_per.reset_index(inplace=True)\n",
    "\n",
    "# Sum numeric columns into a new 'total' row\n",
    "samples_per.loc['total'] = samples_per.sum(axis=0, numeric_only=True)\n",
    "samples_per = samples_per.apply(lambda col: col.astype(int) if pd.api.types.is_numeric_dtype(col) else col) # Turn all numeric columns into ints\n",
    "samples_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df takes up {df.memory_usage().sum() / (2 ** 20):.2f}MB of memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overvoltage Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify outlier frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hem_for_this_cell = 'right'\n",
    "vcvs_df = df.query('lead_location == \"VC/VS\"').dropna(subset=f'lfp_{hem_for_this_cell}_raw')\n",
    "vcvs_df.groupby('pt_id')[f'is_outlier_{hem_for_this_cell}'].mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time of peak per day of each outlier handling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=6\n",
    "df['lfp_left_OvER_interpolate_z_scored_gaussian_smoothed'] = df.groupby(['pt_id', 'days_since_dbs'], group_keys=False)['lfp_left_OvER_interpolate_z_scored'].transform(lambda x: utils.gaussian_smooth(x, sigma=sigma))\n",
    "df['lfp_left_threshold_interpolate_z_scored_gaussian_smoothed'] = df.groupby(['pt_id', 'days_since_dbs'], group_keys=False)['lfp_left_threshold_interpolate_z_scored'].transform(lambda x: utils.gaussian_smooth(x, sigma=sigma))\n",
    "\n",
    "pts_w_outliers = ['B009', 'B012', 'B015', 'B020'] # we leave out B002 because they don't have very much left hemisphere data\n",
    "df_outliers = df.query('pt_id in @pts_w_outliers and lead_location == \"VC/VS\"')\n",
    "peak_times_OvER, peak_times_threshold = [], []\n",
    "for (pt_id, days_since_dbs), day_df in df_outliers.groupby(['pt_id', 'days_since_dbs']):\n",
    "    peak_val_OvER = day_df['lfp_left_OvER_interpolate_z_scored_gaussian_smoothed'].max()\n",
    "    peak_time_OvER = day_df.loc[day_df['lfp_left_OvER_interpolate_z_scored_gaussian_smoothed'] == peak_val_OvER, 'CT_timestamp'].iloc[0]\n",
    "    time_seconds = peak_time_OvER.time().hour * 3600 + peak_time_OvER.time().minute * 60 + peak_time_OvER.time().second\n",
    "    peak_times_OvER.append([time_seconds / (24*3600), peak_val_OvER])\n",
    "\n",
    "    peak_val_threshold = day_df['lfp_left_threshold_interpolate_z_scored_gaussian_smoothed'].max()\n",
    "    peak_time_threshold = day_df.loc[day_df['lfp_left_threshold_interpolate_z_scored_gaussian_smoothed'] == peak_val_threshold, 'CT_timestamp'].iloc[0]\n",
    "    time_seconds = peak_time_threshold.time().hour * 3600 + peak_time_threshold.time().minute * 60 + peak_time_threshold.time().second\n",
    "    peak_times_threshold.append([time_seconds / (24*3600), peak_val_threshold])\n",
    "peak_times_OvER = np.vstack(peak_times_OvER)\n",
    "peak_times_threshold = np.vstack(peak_times_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosinor(t, M, A, phi):\n",
    "    omega = 2 * np.pi / 24\n",
    "    return M + A * np.cos(omega * t + phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = 2 * np.pi / 24\n",
    "window_days = 5\n",
    "half_window = pd.Timedelta(days=window_days / 2)\n",
    "\n",
    "peak_times_OvER = []\n",
    "for (pt_id, lead_location), group_df in df_outliers.groupby(['pt_id', 'lead_location']):\n",
    "    group_df = group_df.sort_values('CT_timestamp').reset_index(drop=True)\n",
    "    unique_days = group_df['CT_timestamp'].dt.floor('D').unique()\n",
    "    p_val_amps = []\n",
    "    for center_day in unique_days:\n",
    "        window_start = center_day + pd.Timedelta(hours=12) - half_window\n",
    "        window_end = center_day + pd.Timedelta(hours=12) + half_window\n",
    "\n",
    "        window_df = group_df[(group_df['CT_timestamp'] >= window_start) & (group_df['CT_timestamp'] < window_end)]\n",
    "\n",
    "        t_hours = window_df['CT_timestamp'].dt.hour + window_df['CT_timestamp'].dt.minute / 60 + window_df['CT_timestamp'].dt.second / 3600\n",
    "        y = window_df['lfp_left_OvER_interpolate'].values\n",
    "\n",
    "        mask = ~np.isnan(y)\n",
    "        t_hours = t_hours[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "        if len(y) < 10:\n",
    "            continue\n",
    "\n",
    "        # Initial guesses: M=mean, A=half range, phi=0\n",
    "        M0 = np.mean(y)\n",
    "        A0 = (np.max(y) - np.min(y)) / 2\n",
    "        phi0 = 0\n",
    "\n",
    "        try:\n",
    "            popt, pcov = curve_fit(cosinor, t_hours, y, p0=[M0, A0, phi0])\n",
    "            perr = np.sqrt(np.diag(pcov))\n",
    "            n = len(y)\n",
    "            p = len(popt)\n",
    "            dof = max(0, n-p)\n",
    "            t_stats = popt / perr\n",
    "            p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), dof))  # two-tailed p-values\n",
    "            p_val_amps.append(p_values[1])  # p-value for amplitude\n",
    "            M_fit, A_fit, phi_fit = popt\n",
    "\n",
    "            if A_fit < 0:\n",
    "                A_fit = -A_fit\n",
    "                phi_fit += np.pi\n",
    "\n",
    "            # Time of peak: when cos is 1 => ωt + φ = 0 mod 2π => t_peak = -φ / ω\n",
    "            omega = 2 * np.pi / 24\n",
    "            t_peak = (-phi_fit % (2 * np.pi)) / omega  # in hours\n",
    "\n",
    "            # Convert t_peak to timestamp on the same date as day_df\n",
    "            date = day_df['CT_timestamp'].iloc[0].normalize()\n",
    "            peak_timestamp = pd.Timestamp(center_day) + pd.to_timedelta(t_peak, unit='h')\n",
    "\n",
    "            peak_times_OvER.append([t_peak * 3600 / (24 * 3600), M_fit + np.abs(A_fit), pt_id])  # Store peak time and amplitude\n",
    "\n",
    "        except RuntimeError:\n",
    "            # fitting failed\n",
    "            continue\n",
    "    print(pt_id, lead_location)\n",
    "    print(f'Min p: {np.min(p_val_amps):.4g}')\n",
    "    print(f'Mean p: {np.mean(p_val_amps):.4g}')\n",
    "    print(f'Max p: {np.max(p_val_amps):.4g}')\n",
    "    print()\n",
    "\n",
    "peak_times_OvER = np.vstack(peak_times_OvER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_times_threshold = []\n",
    "for (pt_id, lead_location), group_df in df_outliers.groupby(['pt_id', 'lead_location']):\n",
    "    group_df = group_df.sort_values('CT_timestamp').reset_index(drop=True)\n",
    "    unique_days = group_df['CT_timestamp'].dt.floor('D').unique()\n",
    "    p_val_amps = []\n",
    "    for center_day in unique_days:\n",
    "        window_start = center_day + pd.Timedelta(hours=12) - half_window\n",
    "        window_end = center_day + pd.Timedelta(hours=12) + half_window\n",
    "\n",
    "        window_df = group_df[(group_df['CT_timestamp'] >= window_start) & (group_df['CT_timestamp'] < window_end)]\n",
    "\n",
    "        t_hours = window_df['CT_timestamp'].dt.hour + window_df['CT_timestamp'].dt.minute / 60 + window_df['CT_timestamp'].dt.second / 3600\n",
    "        y = window_df['lfp_left_threshold_interpolate'].values\n",
    "\n",
    "        mask = ~np.isnan(y)\n",
    "        t_hours = t_hours[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "        if len(y) < 10:\n",
    "            continue\n",
    "\n",
    "        # Initial guesses: M=mean, A=half range, phi=0\n",
    "        M0 = np.mean(y)\n",
    "        A0 = (np.max(y) - np.min(y)) / 2\n",
    "        phi0 = 0\n",
    "\n",
    "        try:\n",
    "            popt, pcov = curve_fit(cosinor, t_hours, y, p0=[M0, A0, phi0])\n",
    "            perr = np.sqrt(np.diag(pcov))\n",
    "            n = len(y)\n",
    "            p = len(popt)\n",
    "            dof = max(0, n-p)\n",
    "            t_stats = popt / perr\n",
    "            p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), dof))  # two-tailed p-values\n",
    "            p_val_amps.append(p_values[1])  # p-value for amplitude\n",
    "            M_fit, A_fit, phi_fit = popt\n",
    "\n",
    "            if A_fit < 0:\n",
    "                A_fit = -A_fit\n",
    "                phi_fit += np.pi\n",
    "\n",
    "            # Time of peak: when cos is 1 => ωt + φ = 0 mod 2π => t_peak = -φ / ω\n",
    "            omega = 2 * np.pi / 24\n",
    "            t_peak = (-phi_fit % (2 * np.pi)) / omega  # in hours\n",
    "\n",
    "            # Convert t_peak to timestamp on the same date as day_df\n",
    "            date = day_df['CT_timestamp'].iloc[0].normalize()\n",
    "            peak_timestamp = pd.Timestamp(center_day) + pd.to_timedelta(t_peak, unit='h')\n",
    "\n",
    "            peak_times_threshold.append([t_peak * 3600 / (24 * 3600), M_fit + np.abs(A_fit), pt_id])  # Store peak time and amplitude\n",
    "\n",
    "        except RuntimeError:\n",
    "            # fitting failed\n",
    "            continue\n",
    "    print(pt_id, lead_location)\n",
    "    print(f'Min p: {np.min(p_val_amps):.4g}')\n",
    "    print(f'Mean p: {np.mean(p_val_amps):.4g}')\n",
    "    print(f'Max p: {np.max(p_val_amps):.4g}')\n",
    "    print()\n",
    "\n",
    "peak_times_threshold = np.vstack(peak_times_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4), subplot_kw=dict(projection='polar'))\n",
    "ax.scatter(peak_times_threshold[:, 0].astype(float) * 2 * np.pi, peak_times_threshold[:, 1].astype(float), label='Threshold Peaks', s=10, marker='o', alpha=0.5)\n",
    "ax.scatter(peak_times_OvER[:, 0].astype(float) * 2 * np.pi, peak_times_OvER[:, 1].astype(float), label='OvER Peaks', s=10, marker='o', alpha=0.5)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_theta_zero_location('N')\n",
    "ax.set(xticks=np.linspace(0, 2*np.pi, 8, endpoint=False), xticklabels=['', '03:00', '06:00', '09:00', '12:00', '15:00', '18:00', '21:00'],\n",
    "       yscale='log', ylim=[1, ax.get_ylim()[1]])\n",
    "plt.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=len(pts_w_outliers), figsize=(3 * len(pts_w_outliers), 4), subplot_kw=dict(projection='polar'))\n",
    "for ax, pt_id in zip(axs.flatten(), pts_w_outliers):\n",
    "    pt_peak_times_OvER = peak_times_OvER[peak_times_OvER[:, 2] == pt_id, :2].astype(float)\n",
    "    pt_peak_times_threshold = peak_times_threshold[peak_times_threshold[:, 2] == pt_id, :2].astype(float)\n",
    "    \n",
    "    ax.scatter(pt_peak_times_threshold[:, 0].astype(float) * 2 * np.pi, pt_peak_times_threshold[:, 1].astype(float), label='Threshold Peaks', s=10, marker='o', alpha=0.5)\n",
    "    ax.scatter(pt_peak_times_OvER[:, 0].astype(float) * 2 * np.pi, pt_peak_times_OvER[:, 1].astype(float), label='OvER Peaks', s=10, marker='o', alpha=0.5)\n",
    "    \n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_zero_location('N')\n",
    "    ax.set(xticks=np.linspace(0, 2*np.pi, 8, endpoint=False), xticklabels=['', '03:00', '06:00', '09:00', '12:00', '15:00', '18:00', '21:00'],)\n",
    "    ax.set_title(pt_id)\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "plt.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and sync Oura ring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using data on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Oura activity and sleep data from server\n",
    "activity_mapping = {\n",
    "    0: 'nonwear',\n",
    "    1: 'rest',\n",
    "    2: 'inactive',\n",
    "    3: 'low activity',\n",
    "    4: 'medium activity',\n",
    "    5: 'high activity'\n",
    "}\n",
    "activity_categories = [activity_mapping[k] for k in sorted(activity_mapping.keys())]\n",
    "activity_dtype = pd.CategoricalDtype(categories=activity_categories, ordered=True)\n",
    "sleep_phase_mapping = {\n",
    "    1: 'deep',\n",
    "    2: 'light',\n",
    "    3: 'REM',\n",
    "    4: 'awake'\n",
    "}\n",
    "sleep_phase_categories = [sleep_phase_mapping[k] for k in sorted(sleep_phase_mapping.keys())]\n",
    "sleep_phase_dtype = pd.CategoricalDtype(categories=sleep_phase_categories, ordered=True)\n",
    "\n",
    "met_dfs, activity_class_dfs, sleep_dfs = [], [], []\n",
    "for pt_id in tqdm(ids):\n",
    "    # Find the JSON file names containing data for the current patient.\n",
    "    pt_oura_dir = Path(patient_dict[pt_id]['data_path']) / '..' / 'oura'\n",
    "\n",
    "    if pt_oura_dir.exists():\n",
    "        for date_dir in pt_oura_dir.iterdir():\n",
    "            activity_path = date_dir / 'daily_activity.json'\n",
    "            if activity_path.exists():\n",
    "                with open(activity_path, 'r') as f:\n",
    "                    activity_data = json.load(f)[0]\n",
    "                met_data = activity_data['met']['items']\n",
    "                met_first_timestamp = pd.to_datetime(datetime.fromisoformat(activity_data['met']['timestamp']))\n",
    "                met_interval = timedelta(seconds=activity_data['met']['interval'])\n",
    "                met_timestamps = met_first_timestamp + met_interval * np.arange(len(met_data))\n",
    "                day_met_df = pd.DataFrame({'timestamp': met_timestamps, 'met_score': met_data, 'pt_id': pt_id})\n",
    "                met_dfs.append(day_met_df)\n",
    "\n",
    "                activity_class_data = [int(c) for c in list(activity_data['class_5_min'])]\n",
    "                activity_class_first_timestamp = pd.to_datetime(datetime.fromisoformat(activity_data['timestamp']))\n",
    "                activity_class_interval = timedelta(minutes=5)\n",
    "                activity_class_timestamps = activity_class_first_timestamp + activity_class_interval * np.arange(len(activity_class_data))\n",
    "                day_activity_class_df = pd.DataFrame({'timestamp': activity_class_timestamps, 'activity_class': activity_class_data, 'pt_id': pt_id})\n",
    "                activity_class_dfs.append(day_activity_class_df)\n",
    "\n",
    "            sleep_path = date_dir / 'sleep.json'\n",
    "            if sleep_path.exists():\n",
    "                with open(sleep_path, 'r') as f:\n",
    "                    sleep_data = json.load(f)\n",
    "                for individual_sleep_data in sleep_data:\n",
    "                    sleep_phases = [int(p) for p in list(individual_sleep_data['sleep_phase_5_min'])]\n",
    "                    sleep_first_timestamp = pd.to_datetime(datetime.fromisoformat(individual_sleep_data['bedtime_start']))\n",
    "                    sleep_interval = timedelta(minutes=5)\n",
    "                    sleep_timestamps = sleep_first_timestamp + sleep_interval * np.arange(len(sleep_phases))\n",
    "                    day_sleep_df = pd.DataFrame({'timestamp': sleep_timestamps, 'sleep_phase': sleep_phases, 'pt_id': pt_id})\n",
    "                    sleep_dfs.append(day_sleep_df)\n",
    "\n",
    "met_df = pd.concat(met_dfs, ignore_index=True)\n",
    "met_df.sort_values(['pt_id', 'timestamp'], inplace=True)\n",
    "activity_class_df = pd.concat(activity_class_dfs, ignore_index=True)\n",
    "activity_class_df['activity_class'] = activity_class_df['activity_class'].map(activity_mapping).astype(activity_dtype)\n",
    "activity_class_df.sort_values(['pt_id', 'timestamp'], inplace=True)\n",
    "sleep_df = pd.concat(sleep_dfs, ignore_index=True)\n",
    "sleep_df['sleep_phase'] = sleep_df['sleep_phase'].map(sleep_phase_mapping).astype(sleep_phase_dtype)\n",
    "sleep_df.sort_values(['pt_id', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up timestamps in CT and UTC for all dataframes.\n",
    "CT_convert = np.vectorize(lambda x: x.tz_convert('America/Chicago') if (pd.notna(x) and x.tzinfo) else pd.NaT)\n",
    "get_orig_time_zone = np.vectorize(lambda x: x.tzname() if (pd.notna(x) and x.tzinfo) else None)\n",
    "\n",
    "met_df['raw_timestamp'] = met_df['timestamp'].copy()  # Save the original timestamp before converting to CT\n",
    "met_df['CT_timestamp'] = CT_convert(met_df['timestamp'])\n",
    "met_df['original_time_zone'] = get_orig_time_zone(met_df['timestamp'])\n",
    "met_df['timestamp'] = met_df['CT_timestamp'].dt.tz_convert('UTC')\n",
    "\n",
    "activity_class_df['raw_timestamp'] = activity_class_df['timestamp'].copy()  # Save the original timestamp before converting to CT\n",
    "activity_class_df['CT_timestamp'] = CT_convert(activity_class_df['timestamp'])\n",
    "activity_class_df['original_time_zone'] = get_orig_time_zone(activity_class_df['timestamp'])\n",
    "activity_class_df['timestamp'] = activity_class_df['CT_timestamp'].dt.tz_convert('UTC')\n",
    "\n",
    "sleep_df['raw_timestamp'] = sleep_df['timestamp'].copy()  # Save the original timestamp before converting to CT\n",
    "sleep_df['CT_timestamp'] = CT_convert(sleep_df['timestamp'])\n",
    "sleep_df['original_time_zone'] = get_orig_time_zone(sleep_df['timestamp']) # save the time zone the data was collected in so we know for sure where they were located\n",
    "sleep_df['timestamp'] = sleep_df['CT_timestamp'].dt.tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_oura_data(pt_lfp_df, pt_oura_df, oura_val_colname, new_col_name, nonwear_fill_val=[0], get_orig_time_zone=True):\n",
    "    \"\"\"\n",
    "    Aligns Oura data with LFP data for a given patient.\n",
    "\n",
    "    Parameters:\n",
    "    - pt_lfp_df (pd.DataFrame): DataFrame containing LFP data.\n",
    "    - pt_oura_df (pd.DataFrame): DataFrame containing Oura data.\n",
    "    - oura_val_colname (str): Column name in Oura data to align with LFP data.\n",
    "    - new_col_name (str): New column name for the aligned Oura data.\n",
    "    - nonwear_fill_val (Object, optional): Value to fill for non-wear periods.\n",
    "    - get_orig_time_zone (bool, optional): Whether to include the original time zone in the output.\n",
    "    \"\"\"\n",
    "    oura_interval_starts = pt_oura_df['timestamp'].values\n",
    "    oura_interval_ends = pt_oura_df['timestamp'].values + pd.Timedelta(minutes=1)\n",
    "    oura_intervals = np.vstack((oura_interval_starts, oura_interval_ends)).T\n",
    "\n",
    "    lfp_interval_starts = pt_lfp_df['timestamp'].values - pd.Timedelta(minutes=10)\n",
    "    lfp_interval_ends = pt_lfp_df['timestamp'].values\n",
    "    lfp_intervals = np.vstack((lfp_interval_starts, lfp_interval_ends)).T\n",
    "\n",
    "    lfp_starts = lfp_intervals[:, 0]\n",
    "    lfp_ends = lfp_intervals[:, 1]\n",
    "    oura_starts = oura_intervals[:, 0]\n",
    "    oura_ends = oura_intervals[:, 1]\n",
    "\n",
    "    upper = np.searchsorted(oura_starts, lfp_ends, side='right')\n",
    "    lower = np.searchsorted(oura_ends, lfp_starts, side='left')\n",
    "\n",
    "    oura_vals, time_zones = [], []\n",
    "    for i in range(len(lfp_starts)):\n",
    "        l, u = lower[i], upper[i]\n",
    "        if l < u:\n",
    "            candidate_idx = np.arange(l, u)\n",
    "            slice_ends2 = oura_ends[l:u]\n",
    "            slice_starts2 = oura_starts[l:u]\n",
    "            mask = (slice_ends2 > lfp_starts[i]) & (slice_starts2 < lfp_ends[i])\n",
    "            overlapping_indices = candidate_idx[mask]\n",
    "        else:\n",
    "            overlapping_indices = np.array([], dtype=int)\n",
    "        oura_vals.append([pt_oura_df[oura_val_colname].iloc[overlapping_indices].values.tolist()] if len(overlapping_indices) > 0 else [nonwear_fill_val])\n",
    "        if get_orig_time_zone:\n",
    "            time_zone_set = set(pt_oura_df['original_time_zone'].iloc[overlapping_indices].values.tolist())\n",
    "            time_zones.append(time_zone_set)\n",
    "    \n",
    "    return_df = pd.DataFrame(oura_vals, columns=[new_col_name], index=pt_lfp_df.index)\n",
    "    if get_orig_time_zone:\n",
    "        return_df['original_time_zone'] = time_zones\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync up the Oura data with the LFP data for activity and sleep by combining the two dataframes.\n",
    "lfp_groups = df.groupby(['pt_id'], group_keys=False)\n",
    "met_vals_df = lfp_groups.apply(lambda g: align_oura_data(g, met_df.query('pt_id == @g.name'), 'met_score', new_col_name='met_vals', nonwear_fill_val=[0]), include_groups=False)\n",
    "activity_vals_df = lfp_groups.apply(lambda g: align_oura_data(g, activity_class_df.query('pt_id == @g.name'), 'activity_class', new_col_name='activity_classes', nonwear_fill_val=['nonwear']), include_groups=False)\n",
    "sleep_phases_df = lfp_groups.apply(lambda g: align_oura_data(g, sleep_df.query('pt_id == @g.name'), 'sleep_phase', new_col_name='sleep_phases', nonwear_fill_val=[]), include_groups=False)\n",
    "df['met_vals'] = met_vals_df['met_vals']\n",
    "df['activity_classes'] = activity_vals_df['activity_classes']\n",
    "df['sleep_phases'] = sleep_phases_df['sleep_phases']\n",
    "\n",
    "df['original_time_zones'] = [s1 | s2 | s3 for s1, s2, s3 in zip(met_vals_df['original_time_zone'], activity_vals_df['original_time_zone'], sleep_phases_df['original_time_zone'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_met'] = df['met_vals'].apply(lambda x: max(x))\n",
    "df['avg_met'] = df['met_vals'].apply(lambda x: np.mean(x))\n",
    "df['wearing_ring'] = df['activity_classes'].apply(lambda x: 'nonwear' not in x)\n",
    "nanmet_inds = np.where(df['max_met'].isna())[0]\n",
    "df.loc[df.index[nanmet_inds], 'max_met'] = pd.Series([0] * len(nanmet_inds), index=df.index[nanmet_inds])\n",
    "df.loc[df.index[nanmet_inds], 'wearing_ring'] = pd.Series([False] * len(nanmet_inds), index=df.index[nanmet_inds])\n",
    "\n",
    "# Fill in sleep data in more interpretable way\n",
    "unknown_mask = ~df['wearing_ring']\n",
    "awake_mask = df['wearing_ring'] & ((df['sleep_phases'].apply(lambda x: set(x) == {'awake'})) | (df['sleep_phases'].apply(lambda x: x == [])))\n",
    "asleep_mask = df['wearing_ring'] & (~df['sleep_phases'].apply(lambda x: 'awake' in x)) & (df['sleep_phases'].apply(lambda x: x != []))\n",
    "df['sleep_state'] = 'Mixed'\n",
    "df.loc[unknown_mask, 'sleep_state'] = 'Unknown'\n",
    "df.loc[awake_mask, 'sleep_state'] = 'Awake'\n",
    "df.loc[asleep_mask, 'sleep_state'] = 'Asleep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oura_df = df.query('wearing_ring').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Oura data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oura_df = df.query('wearing_ring == True and lead_location == \"VC/VS\"')\n",
    "oura_samples_per = pd.DataFrame(oura_df['pt_id'].value_counts(sort=False), index=df['pt_id'].unique())\n",
    "oura_samples_per['samples'] = oura_samples_per['count']\n",
    "oura_samples_per.drop(columns=['count'], inplace=True)\n",
    "for pt_id, pt_oura_df in oura_df.groupby('pt_id'):\n",
    "    dbs_on_date = utils.get_dbs_on_date(patient_dict[pt_id]['dbs_on_date'])\n",
    "    oura_samples_per.loc[pt_id, 'unique_days'] = pt_oura_df.groupby(pd.Grouper(key='CT_timestamp', freq='D')).head(1).shape[0]\n",
    "    oura_samples_per.loc[pt_id, 'days pre-DBS'] = pt_oura_df[pt_oura_df['CT_timestamp'].dt.date < dbs_on_date].groupby(pd.Grouper(key='CT_timestamp', freq='D')).head(1).shape[0]\n",
    "    oura_samples_per.loc[pt_id, 'days post-DBS'] = pt_oura_df[pt_oura_df['CT_timestamp'].dt.date > dbs_on_date].groupby(pd.Grouper(key='CT_timestamp', freq='D')).head(1).shape[0]\n",
    "oura_samples_per.fillna(0, inplace=True)\n",
    "oura_samples_per = oura_samples_per.astype(int)\n",
    "oura_samples_per.loc['total'] = oura_samples_per.sum()\n",
    "oura_samples_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show when patients were wearing ring.\n",
    "print('Patient was wearing ring in highlighted regions')\n",
    "\n",
    "ncols = 3\n",
    "df_vcvs = df.query('lead_location == \"VC/VS\"')\n",
    "fig, axs = plt.subplots(nrows=np.ceil(df_vcvs['pt_id'].nunique() / ncols).astype(int), ncols=ncols, figsize=(20,16), sharey=True)\n",
    "for ax, (pt_id, pt_df) in zip(axs.flatten(), df_vcvs.groupby('pt_id')):\n",
    "    ax.scatter(pt_df['CT_timestamp'], pt_df[f'lfp_left_OvER_interpolate_z_scored'], s=2)\n",
    "    utils.transform_timestamp_to_days(pt_df, ax)\n",
    "    ax.set(xlabel='Days since DBS', ylabel='Residual Variance', title=pt_id)\n",
    "\n",
    "    ring_inds = np.where(pt_df['wearing_ring'] == True)[0]\n",
    "    continuous_chunks = np.split(ring_inds, np.where(np.diff(ring_inds) != 1)[0] + 1)\n",
    "    if len(continuous_chunks[0]) == 0:\n",
    "        continue\n",
    "    for chunk in continuous_chunks:\n",
    "        ax.axvspan(pt_df.iloc[chunk[0]]['CT_timestamp'], pt_df.iloc[chunk[-1]]['CT_timestamp'], color='limegreen', alpha=0.3, ec=None)\n",
    "\n",
    "for ax in axs.flatten()[len(df_vcvs.groupby('pt_id')):]:\n",
    "    ax.axis('off')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all activity and sleep data into a separate dataframe that has not been reduced to just that intersecting with neural data.\n",
    "sleep_df['utc_times'] = sleep_df['timestamp'].dt.ceil('5min')\n",
    "met_df['utc_times'] = met_df['timestamp'].dt.ceil('5min')\n",
    "sleep_df['home_time_sleep'] = sleep_df.apply(lambda r: r['utc_times'].tz_convert(r['original_time_zone']).time(), axis=1)\n",
    "met_df['home_time_met'] = met_df.apply(lambda r: r['utc_times'].tz_convert(r['original_time_zone']).time(), axis=1)\n",
    "activity_and_sleep_df = pd.merge(sleep_df[['pt_id', 'utc_times', 'sleep_phase', 'home_time_sleep']], met_df[['pt_id', 'utc_times', 'met_score', 'home_time_met']], on=['pt_id', 'utc_times'], how='outer')\n",
    "activity_and_sleep_df.query('met_score > 0.1', inplace=True) # Remove non-wear data.\n",
    "activity_and_sleep_df.fillna({'sleep_phase': 'awake'}, inplace=True)\n",
    "activity_and_sleep_df.loc[activity_and_sleep_df['home_time_sleep'].isna(), 'home_time_sleep'] = activity_and_sleep_df.loc[activity_and_sleep_df['home_time_sleep'].isna(), 'home_time_met']\n",
    "activity_and_sleep_df['sleep_state'] = np.where(activity_and_sleep_df['sleep_phase'] == 'awake', 'Awake', 'Asleep')\n",
    "activity_and_sleep_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Paper/SfN Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "overvoltage_color = 'crimson'\n",
    "nonovervoltage_color = 'coral'\n",
    "corrected_overvoltage_color = 'dodgerblue'\n",
    "\n",
    "hemi = 'left'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added a lot of \"if True\" statements to be able to fold blocks in VS Code. They don't do anything functionally, purely cosmetic. We made all our figures in Python without the use of a vector editing program like Adobe Illustrator because we wanted to be able to reproduce them easily and quickly. Illustrator also could not handle the number of data points we had in our figures, so we had to use Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1.\n",
    "\n",
    "fig = plt.figure(figsize=(13, 8))\n",
    "gs = gridspec.GridSpec(nrows=3, ncols=2, figure=fig, height_ratios=[2,2, 1], width_ratios=[4, 1])\n",
    "\n",
    "# Fig A-B: Scatter plots for patients B009 and B004\n",
    "if True:\n",
    "    subplot_AB_gs = gs[:-1, :-1].subgridspec(2, 1, hspace=0.4)\n",
    "    axA = fig.add_subplot(subplot_AB_gs[0, 0])\n",
    "    axB = fig.add_subplot(subplot_AB_gs[1, 0], sharey=axA)\n",
    "\n",
    "    # Fig A: Draw scatter plot for patient B009 (3387 leads)\n",
    "    if True:\n",
    "        s = 0.15\n",
    "        id = 'B009'\n",
    "        pt_df = df.query(f'pt_id == @id and lfp_{hemi}_raw > 15')\n",
    "        non_outlier_pts = pt_df.query(f'is_outlier_{hemi} == False')\n",
    "        axA.scatter(non_outlier_pts['CT_timestamp'], non_outlier_pts[f'lfp_{hemi}_raw'], s=s, c=nonovervoltage_color, label='_nolegend_')\n",
    "        outlier_pts = pt_df.query(f'is_outlier_{hemi} == True')\n",
    "        axA.scatter(outlier_pts['CT_timestamp'], outlier_pts[f'lfp_{hemi}_raw'], s=s, c=overvoltage_color, label='_nolegend_')\n",
    "\n",
    "        axA_twin = axA.twinx()\n",
    "        daily_outlier_percentage = pt_df.groupby(pd.Grouper(key='CT_timestamp', freq='D'))[f'is_outlier_{hemi}'].mean() * 100\n",
    "        axA_twin.plot(daily_outlier_percentage.index, daily_outlier_percentage.values, color='darkslateblue', lw=1)\n",
    "\n",
    "        axA.set(xlabel='Days Since DBS', ylabel='LFP Power', yscale='log', title=f'{id} (3387 Leads)')\n",
    "        axA.yaxis.label.set_color('k')\n",
    "        axA.tick_params(axis='y', colors='k', which='both')\n",
    "        axA_twin.set(ylabel='Daily Overvoltage Percentage (%)', ylim=[-3, 100])\n",
    "        axA_twin.yaxis.label.set_color('darkslateblue')\n",
    "        axA_twin.tick_params(axis='y', colors='darkslateblue', which='both')\n",
    "        utils.transform_timestamp_to_days(pt_df, axA)\n",
    "\n",
    "    # Fig A callout: zoom in on outliers\n",
    "    if True:\n",
    "        callout_df = pt_df.query('days_since_dbs > 1530')\n",
    "        subplot_A_callout = gs[0, -1].subgridspec(1, 1)\n",
    "        axA_callout = fig.add_subplot(subplot_A_callout[0, 0])\n",
    "        axA_callout.scatter(callout_df.query('is_outlier_left == True')['CT_timestamp'],\n",
    "                            callout_df.query('is_outlier_left == True')['lfp_left_raw'],\n",
    "                            s=0.1, c=overvoltage_color, label='_nolegend_')\n",
    "        axA_callout.scatter(callout_df.query('is_outlier_left == False')['CT_timestamp'],\n",
    "                            callout_df.query('is_outlier_left == False')['lfp_left_raw'],\n",
    "                            s=0.1, c=nonovervoltage_color, label='_nolegend_')\n",
    "        utils.transform_timestamp_to_days(callout_df, axA_callout)\n",
    "        axA_callout.ticklabel_format(style='scientific', axis='y', scilimits=(0, 0))\n",
    "        axA_callout.set(xlabel='Days Since DBS', ylabel='LFP Power', title=f'{id}', ylim=[-1e8, axA_callout.get_ylim()[1]],\n",
    "                        yticks=[0, 1e9, 2e9], yticklabels=['0', r'10$^\\text{9}$', r'2$\\times$10$^\\text{9}$'])\n",
    "        \n",
    "        axA_callout_right = axA_callout.twinx()\n",
    "        yticks = np.linspace(0, 2**32-1, 61, endpoint=True)[::5]\n",
    "        axA_callout_right.set_yticks(yticks)\n",
    "        axA_callout_right.set_yticklabels(np.arange(0, 61, 5))\n",
    "        axA_callout_right.set_ylim(axA_callout.get_ylim())\n",
    "        axA_callout_right.set_ylabel('Number of Overvoltages per Interval')\n",
    "\n",
    "        [spine.set_color('b') for spine in axA_callout.spines.values()]\n",
    "        [spine.set_color('b') for spine in axA_callout_right.spines.values()]\n",
    "\n",
    "        y_start_callout_box = 1e4\n",
    "        rect = mpatches.Rectangle((axA.get_xlim()[1]-65, y_start_callout_box), 46, 3e9, color='b', zorder=0, lw=0.5, fill=False)\n",
    "        axA.add_patch(rect)\n",
    "\n",
    "    # Fig B: Draw scatter plot for patient B004 (SenSight leads)\n",
    "    if True:\n",
    "        id = 'B004'\n",
    "        pt_df = df.query(f'pt_id == @id and lfp_{hemi}_raw > 15')\n",
    "        non_outlier_pts = pt_df.query(f'is_outlier_{hemi} == False')\n",
    "        axB.scatter(non_outlier_pts['CT_timestamp'], non_outlier_pts[f'lfp_{hemi}_raw'], s=s, c=nonovervoltage_color, label='_nolegend_')\n",
    "        outlier_pts = pt_df.query(f'is_outlier_{hemi} == True')\n",
    "        axB.scatter(outlier_pts['CT_timestamp'], outlier_pts[f'lfp_{hemi}_raw'], s=s, c=overvoltage_color, label='_nolegend_')\n",
    "\n",
    "        axB_twin = axB.twinx()\n",
    "        daily_outlier_percentage = pt_df.groupby(pd.Grouper(key='CT_timestamp', freq='D'))[f'is_outlier_{hemi}'].mean() * 100\n",
    "        axB_twin.plot(daily_outlier_percentage.index, daily_outlier_percentage.values, color='darkslateblue', lw=1)\n",
    "\n",
    "        axB.set(xlabel='Days Since DBS', ylabel='LFP Power', yscale='log', title=f'{id} (SenSight Leads)')\n",
    "        axB.yaxis.label.set_color('k')\n",
    "        axB.tick_params(axis='y', colors='k', which='both')\n",
    "        axB_twin.set(ylabel='Daily Overvoltage Percentage (%)', ylim=[-3, 100])\n",
    "        axB_twin.yaxis.label.set_color('darkslateblue')\n",
    "        axB_twin.tick_params(axis='y', colors='darkslateblue', which='both')\n",
    "        utils.transform_timestamp_to_days(pt_df, axB)\n",
    "\n",
    "# Fig C: Box plots\n",
    "if True:\n",
    "    subplot_C_gs = gs[1:, -1:].subgridspec(2, 1, height_ratios=[4, 1])\n",
    "    axC, axC_inset = fig.add_subplot(subplot_C_gs[0, 0]), fig.add_subplot(subplot_C_gs[1, 0])\n",
    "    noise_amt, hem_spread = 0.03, 0.02\n",
    "\n",
    "    # Fig C: Draw box plots for overvoltage percentage by lead model\n",
    "    if True:\n",
    "        lead_model_vals = []\n",
    "        for i, (lead_model, lead_df) in enumerate(df.query('lead_location == \"VC/VS\"').groupby('lead_model')):\n",
    "            lead_outlier_percents_left, lead_outlier_percents_right = [], []\n",
    "            for pt_id, pt_df in lead_df.groupby('pt_id'):\n",
    "                if not pt_df.dropna(subset=['lfp_left_raw']).empty:\n",
    "                    left_outlier_percentage = pt_df.dropna(subset=['lfp_left_raw'])['is_outlier_left'].mean()\n",
    "                    lead_outlier_percents_left.append(left_outlier_percentage * 100)\n",
    "                if not pt_df.dropna(subset=['lfp_right_raw']).empty:\n",
    "                    right_outlier_percentage = pt_df.dropna(subset=['lfp_right_raw'])['is_outlier_right'].mean()\n",
    "                    lead_outlier_percents_right.append(right_outlier_percentage * 100)\n",
    "            lead_outlier_percents = np.concatenate([lead_outlier_percents_left, lead_outlier_percents_right])\n",
    "            axC.boxplot(lead_outlier_percents, positions=[i/2], whis=100000000, zorder=5)\n",
    "            hor_noise = np.random.RandomState(42).uniform(-noise_amt, noise_amt, len(lead_outlier_percents))# * i\n",
    "            axC.scatter(np.array([i/2] * len(lead_outlier_percents_left)) - hem_spread + np.random.RandomState(42).uniform(-noise_amt, noise_amt, len(lead_outlier_percents_left)) * i,\n",
    "                        lead_outlier_percents_left, s=15, alpha=0.7, c='darkgreen', edgecolor='k', zorder=10)\n",
    "            axC.scatter(np.array([i/2] * len(lead_outlier_percents_right)) + hem_spread + np.random.RandomState(42).uniform(-noise_amt, noise_amt, len(lead_outlier_percents_right)) * i,\n",
    "                        lead_outlier_percents_right, s=15, alpha=0.7, c='darkgoldenrod', edgecolor='k', zorder=10)\n",
    "            lead_model_vals.append(lead_outlier_percents)\n",
    "        old_leads, new_leads, lead_location = 'LEAD_3387', 'LEAD_B33015', 'VC/VS'\n",
    "        axC.set(xticks=[0,1/2],\n",
    "                xticklabels=[f'3387 Lead\\n(N={df.query(\"lead_model == @old_leads and lead_location == @lead_location\")[\"pt_id\"].nunique()})',\n",
    "                             f'SenSight Lead\\n(N={df.query(\"lead_model == @new_leads and lead_location == @lead_location\")[\"pt_id\"].nunique()})'],\n",
    "                ylabel='Overvoltage Percentage (%)',\n",
    "                xlim=[-.2, 0.7])\n",
    "        rect = mpatches.Rectangle((axC.get_xlim()[0], -.2), (axC.get_xlim()[1] - axC.get_xlim()[0]), 0.8, color='b', zorder=0, lw=0.5, fill=False)\n",
    "        axC.add_patch(rect)\n",
    "\n",
    "        u_stat, p_value = stats.mannwhitneyu(lead_model_vals[0], lead_model_vals[1], alternative='two-sided')\n",
    "        axC.text(0.65, 0.65, f'p={p_value:.3g}', transform=axC.transAxes, ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # Fig C inset: Draw inset box plots for overvoltage percentage by lead model close to 0\n",
    "    if True:\n",
    "        lead_model_vals = []\n",
    "        for i, (lead_model, lead_df) in enumerate(df.query('lead_location == \"VC/VS\"').groupby('lead_model')):\n",
    "            lead_outlier_percents_left, lead_outlier_percents_right = [], []\n",
    "            for pt_id, pt_df in lead_df.groupby('pt_id'):\n",
    "                if not pt_df.dropna(subset=['lfp_left_raw']).empty:\n",
    "                    left_outlier_percentage = pt_df.dropna(subset=['lfp_left_raw'])['is_outlier_left'].mean()\n",
    "                    lead_outlier_percents_left.append(left_outlier_percentage * 100)\n",
    "                if not pt_df.dropna(subset=['lfp_right_raw']).empty:\n",
    "                    right_outlier_percentage = pt_df.dropna(subset=['lfp_right_raw'])['is_outlier_right'].mean()\n",
    "                    lead_outlier_percents_right.append(right_outlier_percentage * 100)\n",
    "            lead_outlier_percents = np.concatenate([lead_outlier_percents_left, lead_outlier_percents_right])\n",
    "            axC_inset.boxplot(lead_outlier_percents, positions=[i/2], whis=100000000, zorder=5)\n",
    "            hor_noise = np.random.RandomState(42).uniform(-noise_amt, noise_amt, len(lead_outlier_percents))# * i\n",
    "            axC_inset.scatter(np.array([i/2] * len(lead_outlier_percents_left)) - hem_spread + np.random.RandomState(42).uniform(-noise_amt, noise_amt, len(lead_outlier_percents_left)) * i,\n",
    "                        lead_outlier_percents_left, s=15, alpha=0.7, c='darkgreen', edgecolor='k', zorder=10)\n",
    "            axC_inset.scatter(np.array([i/2] * len(lead_outlier_percents_right)) + hem_spread + np.random.RandomState(42).uniform(-noise_amt, noise_amt, len(lead_outlier_percents_right)) * i,\n",
    "                        lead_outlier_percents_right, s=15, alpha=0.7, c='darkgoldenrod', edgecolor='k', zorder=10)\n",
    "            lead_model_vals.append(lead_outlier_percents)\n",
    "        axC_inset.set(ylim=[-0.05, 0.25], xticks=[0,1/2], xticklabels=[f'3387 Lead', f'SenSight Lead'], xlim=[-.2, 0.7])\n",
    "        [spine.set_color('b') for spine in axC_inset.spines.values()]\n",
    "\n",
    "# Fig D: Polar plots for patients with high outlier percentage\n",
    "if True:\n",
    "    high_outlier_ids = ['B009', 'B012', 'B015', 'B020']\n",
    "    subplot_D_gs = gs[-1, :-1].subgridspec(1, len(high_outlier_ids)+1, width_ratios=[1] * len(high_outlier_ids) + [0.3])\n",
    "    axs = [fig.add_subplot(subplot_D_gs[0, i], polar=True) for i in range(len(high_outlier_ids))]\n",
    "\n",
    "    # Fig D: Draw polar plots for patients with high outlier percentage\n",
    "    if True:\n",
    "        for i, (ax, id) in enumerate(zip(axs, high_outlier_ids)):\n",
    "            pt_df = df.query('pt_id == @id and (pt_id != \"B015\" or days_since_dbs >= 830)').copy()\n",
    "            pt_df['times'] = pt_df['CT_timestamp'].dt.tz_convert('UTC').dt.ceil('10min').dt.tz_convert('US/Central').dt.time # have to sidestep to UTC to avoid issues with daylight savings time\n",
    "\n",
    "            if id == \"B009\":\n",
    "                pt_df['times'] = pt_df['times'].apply(lambda t: datetime.combine(datetime.today(), t) + timedelta(hours=1)).dt.time\n",
    "            \n",
    "            # Calculate the x and y coordinates for the polar plot\n",
    "            num_left_outliers = pt_df.groupby('times')[f'is_outlier_left'].sum() / pt_df.groupby('times')[f'is_outlier_left'].count()\n",
    "            num_right_outliers = pt_df.groupby('times')[f'is_outlier_right'].sum() / pt_df.groupby('times')[f'is_outlier_right'].count()\n",
    "            num_left_outliers /= num_left_outliers.max() if num_left_outliers.max() > 0 else 1\n",
    "            num_right_outliers /= num_right_outliers.max() if num_right_outliers.max() > 0 else 1\n",
    "            num_wedges = 24*6\n",
    "            angle_offset = 2 * np.pi / num_wedges # Offset by a wedge to correctly place the bars\n",
    "            angles = np.linspace(0 + angle_offset, 2*np.pi + angle_offset, num_wedges, endpoint=False)\n",
    "\n",
    "            # Set midnight at the top and progress clockwise\n",
    "            ax.set_theta_zero_location(\"N\")\n",
    "            ax.set_theta_direction(-1)\n",
    "\n",
    "            # Draw the wedges\n",
    "            for j, (angle, left_val, right_val) in enumerate(zip(angles, num_left_outliers.values, num_right_outliers.values)):\n",
    "                ax.bar(angle, left_val, width=2*np.pi/num_wedges, alpha=1, color='C0', label=('Number of\\nLeft Hemisphere\\nOvervoltages' if j == 0 else '_nolegend_'))\n",
    "                ax.bar(angle, right_val, width=2*np.pi/num_wedges, alpha=0.85, color='C8', label=('Number of\\nRight Hemisphere\\nOvervoltages' if j == 0 else '_nolegend_'))\n",
    "            max_y = max(num_left_outliers.max(), num_right_outliers.max())\n",
    "\n",
    "            # Draw line for when patient was awake\n",
    "            pt_activity_sleep_df = activity_and_sleep_df.query('pt_id == @id').groupby('utc_times').head(3).groupby('utc_times').tail(1)\n",
    "            awake_ratios = {}\n",
    "            for time, group in pt_activity_sleep_df.groupby('home_time_sleep'):\n",
    "                vc = group['sleep_state'].value_counts()\n",
    "                awake_ratios[time] = vc['Awake'] / (vc['Awake'] + vc['Asleep']) if 'Awake' in vc and 'Asleep' in vc else (1 if 'Awake' in vc else 0)\n",
    "            awake_ratios = pd.Series(awake_ratios)\n",
    "            awake_angles = np.linspace(0, 2*np.pi, len(awake_ratios), endpoint=False)\n",
    "            ax.plot(awake_angles, awake_ratios * max_y, color='purple', lw=1, label='Awake Ratio')\n",
    "\n",
    "            # Clean up the plot\n",
    "            ax.set(xticks=np.linspace(0, 2 * np.pi, 8, endpoint=False),\n",
    "                xticklabels=['', '3:00', '6:00', '9:00', '12:00', '15:00', '18:00', '21:00'])\n",
    "            if awake_ratios.notna().any():\n",
    "                ax.set(yticks=[max_y] if awake_ratios.notna().any() else [],\n",
    "                       yticklabels=['100%'] if awake_ratios.notna().any() else [])\n",
    "                ax.get_yticklabels()[0].set_horizontalalignment('right')\n",
    "                ax.get_yticklabels()[0].set_verticalalignment('top')\n",
    "            else:\n",
    "                ax.set(yticks=[], yticklabels=[])\n",
    "            ax.tick_params(axis='y', colors='purple')\n",
    "            ax.set_rlabel_position(245)\n",
    "            ax.set(title=id)\n",
    "\n",
    "# Finish up figure canvas\n",
    "fig.tight_layout()\n",
    "\n",
    "# Display legends\n",
    "if True:\n",
    "    # Fig A legend\n",
    "    if True:\n",
    "        outlier_legend_dot = Line2D([0], [0], marker='o', color=overvoltage_color, label='Overvoltage', markersize=4, linestyle='None')\n",
    "        nonoutlier_legend_dot = Line2D([0], [0], marker='o', color=nonovervoltage_color, label='Nonovervoltage', markersize=4, linestyle='None')\n",
    "        handles = [outlier_legend_dot, nonoutlier_legend_dot, axA_twin.get_lines()[0]]\n",
    "        labels = ['Overvoltage', 'Nonovervoltage', 'Overvoltage Percentage']\n",
    "        A_bottom = axA.get_xticklabels()[0].get_window_extent().transformed(fig.transFigure.inverted()).y0\n",
    "        B_top = axB.get_window_extent().transformed(fig.transFigure.inverted()).y1\n",
    "        A_left = axA.get_window_extent().transformed(fig.transFigure.inverted()).x0\n",
    "        fig.legend(handles,\n",
    "                labels,\n",
    "                bbox_to_anchor=(A_left, (A_bottom + B_top) / 2),\n",
    "                loc='center left',\n",
    "                fontsize=8)\n",
    "    # Fig D legend\n",
    "    if True:\n",
    "        axs[-1].legend(\n",
    "            loc='center left',\n",
    "            bbox_to_anchor=(1.3, 0.5),\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "# Draw lines connecting subfigure to inset subfigure for A.\n",
    "if True:\n",
    "    start_disp = [\n",
    "        axA.transData.transform((axA.get_xlim()[1]-65+46, y_start_callout_box)),\n",
    "        axA.transData.transform((axA.get_xlim()[1]-65+46, y_start_callout_box+3e9))\n",
    "    ]\n",
    "    end_disp = [\n",
    "        axA_callout.transData.transform((axA_callout.get_xlim()[0], axA_callout.get_ylim()[0])),\n",
    "        axA_callout.transData.transform((axA_callout.get_xlim()[0], axA_callout.get_ylim()[1]))\n",
    "    ]\n",
    "    for start, end in zip(start_disp, end_disp):\n",
    "        start_fig = fig.transFigure.inverted().transform(start)\n",
    "        end_fig = fig.transFigure.inverted().transform(end)\n",
    "        line = Line2D([start_fig[0], end_fig[0]],\n",
    "                      [start_fig[1], end_fig[1]],\n",
    "                      lw=0.5, color='b')\n",
    "        fig.add_artist(line)\n",
    "\n",
    "# Draw lines connecting box plot subfigure to inset subfigure for C.\n",
    "if True:\n",
    "    start_disp = [\n",
    "        axC.transData.transform((-.1, -.3)),\n",
    "        axC.transData.transform((0.6, -.3))\n",
    "    ]\n",
    "    end_disp = [\n",
    "        axC_inset.transData.transform((axC_inset.get_xlim()[0], axC_inset.get_ylim()[1])),\n",
    "        axC_inset.transData.transform((axC_inset.get_xlim()[1], axC_inset.get_ylim()[1]))\n",
    "    ]\n",
    "    for start, end in zip(start_disp, end_disp):\n",
    "        start_fig = fig.transFigure.inverted().transform(start)\n",
    "        end_fig = fig.transFigure.inverted().transform(end)\n",
    "        line = Line2D([start_fig[0], end_fig[0]],\n",
    "                      [start_fig[1], end_fig[1]],\n",
    "                      lw=0.5, color='b')\n",
    "        fig.add_artist(line)\n",
    "\n",
    "# Add subfigure labels\n",
    "if True:\n",
    "    vert_offset, hor_offset = 0.01, -0.04\n",
    "    A_left = axA.get_window_extent().transformed(fig.transFigure.inverted()).x0\n",
    "    A_top = axA.get_window_extent().transformed(fig.transFigure.inverted()).y1\n",
    "    B_left = axB.get_window_extent().transformed(fig.transFigure.inverted()).x0\n",
    "    B_top = axB.get_window_extent().transformed(fig.transFigure.inverted()).y1\n",
    "    C_left = axC.get_window_extent().transformed(fig.transFigure.inverted()).x0\n",
    "    C_top = axC.get_window_extent().transformed(fig.transFigure.inverted()).y1\n",
    "    D_left = axs[0].get_window_extent().transformed(fig.transFigure.inverted()).x0\n",
    "    D_top = axs[0].get_window_extent().transformed(fig.transFigure.inverted()).y1\n",
    "    fs=14\n",
    "    fig.text(A_left+hor_offset, A_top+vert_offset, 'a', fontsize=fs, fontweight='bold', color='k', fontdict={'family': 'sans-serif'})\n",
    "    fig.text(B_left+hor_offset, B_top+vert_offset, 'b', fontsize=fs, fontweight='bold', color='k', fontdict={'family': 'sans-serif'})\n",
    "    fig.text(C_left+hor_offset, C_top+vert_offset, 'c', fontsize=fs, fontweight='bold', color='k', fontdict={'family': 'sans-serif'})\n",
    "    fig.text(D_left+hor_offset, D_top+vert_offset, 'd', fontsize=fs, fontweight='bold', color='k', fontdict={'family': 'sans-serif'})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2.\n",
    "violinplot_width_ratio = 0.15\n",
    "inset_width_ratio = 0.4\n",
    "wspace=0.02\n",
    "pfontsize=9\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "gs = gridspec.GridSpec(nrows=2, ncols=3, figure=fig)\n",
    "\n",
    "s1, s2 = 0.15, 1\n",
    "\n",
    "id = 'B020'\n",
    "pt_df = oura_df.query('pt_id == @id')\n",
    "non_outlier_pts = pt_df.query(f'is_outlier_{hemi} == False and `lfp_left_OvER_interpolate` > 14').dropna(subset=[f'lfp_{hemi}_raw', f'lfp_left_OvER_interpolate', f'lfp_left_threshold_interpolate'], how='any')\n",
    "outlier_pts = pt_df.query(f'is_outlier_{hemi} == True and `lfp_left_OvER_interpolate` > 14').dropna(subset=[f'lfp_{hemi}_raw', f'lfp_left_OvER_interpolate', f'lfp_left_threshold_interpolate'], how='any')\n",
    "\n",
    "inset_df = pt_df.query('days_since_dbs >= 1924 and days_since_dbs < 1927').dropna(subset=[f'lfp_{hemi}_raw', f'lfp_left_OvER_interpolate', f'lfp_left_threshold_interpolate'], how='any')\n",
    "non_outlier_pts_inset = inset_df.query(f'is_outlier_{hemi} == False and `lfp_left_OvER_interpolate` > 14').dropna(subset=[f'lfp_{hemi}_raw', f'lfp_left_OvER_interpolate', f'lfp_left_threshold_interpolate'], how='any')\n",
    "outlier_pts_inset = inset_df.query(f'is_outlier_{hemi} == True and `lfp_left_OvER_interpolate` > 14').dropna(subset=[f'lfp_{hemi}_raw', f'lfp_left_OvER_interpolate', f'lfp_left_threshold_interpolate'], how='any')\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "pts_w_outliers = ['B009', 'B012', 'B015', 'B020']\n",
    "        \n",
    "omega = 2 * np.pi / 24\n",
    "window_days = 5\n",
    "half_window = pd.Timedelta(days=window_days / 2)\n",
    "\n",
    "# Fig A (OvER)\n",
    "if True:\n",
    "    # Set up subplot A\n",
    "    subplot_A_gs = gs[0, :2].subgridspec(1, 2, width_ratios=[1, inset_width_ratio], wspace=wspace)\n",
    "    axA1 = fig.add_subplot(subplot_A_gs[0, 0])\n",
    "    axA2 = fig.add_subplot(subplot_A_gs[0, 1], sharey=axA1)\n",
    "\n",
    "    # AI: Scatter plot with OvER\n",
    "    axA1.scatter(non_outlier_pts['CT_timestamp'], non_outlier_pts[f'lfp_left_OvER_interpolate'], s=s1, c=nonovervoltage_color, label='_nolegend_')\n",
    "    axA1.scatter(outlier_pts['CT_timestamp'], outlier_pts[f'lfp_left_OvER_interpolate'], s=s1, c=corrected_overvoltage_color, label='_nolegend_')\n",
    "    axA1.scatter(outlier_pts['CT_timestamp'], outlier_pts[f'lfp_{hemi}_raw'], s=s1, c=overvoltage_color, label='_nolegend_')\n",
    "\n",
    "    # AII: Show inset scatter plot for 3 days\n",
    "    axA2.scatter(non_outlier_pts_inset['CT_timestamp'], non_outlier_pts_inset[f'lfp_left_OvER_interpolate'], s=s2, c=nonovervoltage_color, label='_nolegend_')\n",
    "    axA2.scatter(outlier_pts_inset['CT_timestamp'], outlier_pts_inset[f'lfp_left_OvER_interpolate'], s=s2, c=corrected_overvoltage_color, label='_nolegend_')\n",
    "\n",
    "    # Add smoothed line using Gaussian filter\n",
    "    smoothed = gaussian_filter1d(inset_df[f'lfp_left_OvER_interpolate'], sigma=window_size/2)\n",
    "    axA2.plot(inset_df['CT_timestamp'], smoothed, color='k', lw=0.5, label='_nolegend_')\n",
    "\n",
    "    axA1.set(ylabel='LFP Power', yscale='log', xlabel='Days Since DBS')\n",
    "    axA2.tick_params(axis='y', labelleft=False)\n",
    "    axA2.set(ylabel='', xlabel='Days Since DBS')\n",
    "    utils.transform_timestamp_to_days(pt_df, axA1)\n",
    "    utils.transform_timestamp_to_days(inset_df, axA2)\n",
    "\n",
    "# Fig B (threshold and interpolate)\n",
    "if True:\n",
    "    # Set up subplot B\n",
    "    subplot_B_gs = gs[1, :-1].subgridspec(1, 2, width_ratios=[1, inset_width_ratio], wspace=wspace)\n",
    "    axB1 = fig.add_subplot(subplot_B_gs[0, 0], sharex=axA1, sharey=axA1)\n",
    "    axB2 = fig.add_subplot(subplot_B_gs[0, 1], sharex=axA2, sharey=axB1)\n",
    "\n",
    "    # BI: Scatter plot with OvER\n",
    "    axB1.scatter(non_outlier_pts['CT_timestamp'], non_outlier_pts[f'lfp_left_threshold_interpolate'], s=s1, c=nonovervoltage_color, label='_nolegend_')\n",
    "    axB1.scatter(outlier_pts['CT_timestamp'], outlier_pts[f'lfp_left_threshold_interpolate'], s=s1, c=corrected_overvoltage_color, label='_nolegend_')\n",
    "    axB1.scatter(outlier_pts['CT_timestamp'], outlier_pts[f'lfp_{hemi}_raw'], s=s1, c=overvoltage_color, label='_nolegend_')\n",
    "\n",
    "    # AII: Show inset scatter plot for 3 days\n",
    "    axB2.scatter(non_outlier_pts_inset['CT_timestamp'], non_outlier_pts_inset[f'lfp_left_threshold_interpolate'], s=s2, c=nonovervoltage_color, label='_nolegend_')\n",
    "    axB2.scatter(outlier_pts_inset['CT_timestamp'], outlier_pts_inset[f'lfp_left_threshold_interpolate'], s=s2, c=corrected_overvoltage_color, label='_nolegend_')\n",
    "\n",
    "    # Add 9-sample exponential moving average line\n",
    "    smoothed = gaussian_filter1d(inset_df[f'lfp_left_threshold_interpolate'], sigma=window_size/2)\n",
    "    axB2.plot(inset_df['CT_timestamp'], smoothed, color='k', lw=0.5, label='_nolegend_')\n",
    "\n",
    "    axB1.set(ylabel='LFP Power', yscale='log', xlabel='Days Since DBS', title=' ')\n",
    "    axB2.tick_params(axis='y', labelleft=False)\n",
    "    axB2.set(ylabel='', xlabel='Days Since DBS')\n",
    "    utils.transform_timestamp_to_days(pt_df, axA1)\n",
    "    utils.transform_timestamp_to_days(inset_df, axB2)\n",
    "\n",
    "# Fig C (polar plots showing circadianness of each method)\n",
    "if True:\n",
    "    # Set up subplot C\n",
    "    subplot_C_gs = gs[:, -1].subgridspec(2, 2, wspace=0.7)\n",
    "    subplot_C_axes = []\n",
    "    for i, pt_id in enumerate(pts_w_outliers):\n",
    "        ax = fig.add_subplot(subplot_C_gs[i // 2, i % 2], polar=True)\n",
    "        pt_df = df.query('pt_id == @pt_id')\n",
    "\n",
    "        peak_times_OvER, peak_times_thresh = [], []\n",
    "        unique_days = pt_df['CT_timestamp'].dt.floor('D').unique()\n",
    "        p_val_amps_OvER, p_val_amps_thresh = [], []\n",
    "        for center_day in unique_days:\n",
    "            window_start = center_day + pd.Timedelta(hours=12) - half_window\n",
    "            window_end = center_day + pd.Timedelta(hours=12) + half_window\n",
    "\n",
    "            window_df = pt_df[(pt_df['CT_timestamp'] >= window_start) & (pt_df['CT_timestamp'] < window_end)]\n",
    "\n",
    "            t_hours = window_df['CT_timestamp'].dt.hour + window_df['CT_timestamp'].dt.minute / 60 + window_df['CT_timestamp'].dt.second / 3600\n",
    "            y_OvER = window_df['lfp_left_OvER_interpolate'].values\n",
    "            y_thresh = window_df['lfp_left_threshold_interpolate'].values\n",
    "\n",
    "            mask_OvER = ~np.isnan(y_OvER)\n",
    "            t_hours_OvER = t_hours[mask_OvER]\n",
    "            y_OvER = y_OvER[mask_OvER]\n",
    "            mask_thresh = ~np.isnan(y_thresh)\n",
    "            t_hours_thresh = t_hours[mask_thresh]\n",
    "            y_thresh = y_thresh[mask_thresh]\n",
    "\n",
    "            # Initial guesses: M=mean, A=half range, phi=0\n",
    "            M0_OvER = np.mean(y_OvER)\n",
    "            A0_OvER = (np.max(y_OvER) - np.min(y_OvER)) / 2\n",
    "            phi0_OvER = 0\n",
    "            M0_thresh = np.mean(y_thresh)\n",
    "            A0_thresh = (np.max(y_thresh) - np.min(y_thresh)) / 2\n",
    "            phi0_thresh = 0\n",
    "\n",
    "            try:\n",
    "                popt, pcov = curve_fit(cosinor, t_hours_OvER, y_OvER, p0=[M0_OvER, A0_OvER, phi0_OvER])\n",
    "                perr = np.sqrt(np.diag(pcov))\n",
    "                n = len(y)\n",
    "                p = len(popt)\n",
    "                dof = max(0, n-p)\n",
    "                t_stats = popt / perr\n",
    "                p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), dof))  # two-tailed p-values\n",
    "                p_val_amps_OvER.append(p_values[1])  # p-value for amplitude\n",
    "                M_fit, A_fit, phi_fit = popt\n",
    "\n",
    "                if A_fit < 0:\n",
    "                    A_fit = -A_fit\n",
    "                    phi_fit += np.pi\n",
    "\n",
    "                # Time of peak: when cos is 1 => ωt + φ = 0 mod 2π => t_peak = -φ / ω\n",
    "                omega = 2 * np.pi / 24\n",
    "                t_peak = (-phi_fit % (2 * np.pi)) / omega  # in hours\n",
    "\n",
    "                # Convert t_peak to timestamp on the same date as day_df\n",
    "                date = day_df['CT_timestamp'].iloc[0].normalize()\n",
    "                peak_timestamp = pd.Timestamp(center_day) + pd.to_timedelta(t_peak, unit='h')\n",
    "\n",
    "                peak_times_OvER.append([t_peak * 3600 / (24 * 3600), M_fit + np.abs(A_fit)])  # Store peak time and amplitude\n",
    "            except RuntimeError:\n",
    "                # fitting failed\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                popt, pcov = curve_fit(cosinor, t_hours_thresh, y_thresh, p0=[M0_thresh, A0_thresh, phi0_thresh])\n",
    "                perr = np.sqrt(np.diag(pcov))\n",
    "                n = len(y)\n",
    "                p = len(popt)\n",
    "                dof = max(0, n-p)\n",
    "                t_stats = popt / perr\n",
    "                p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), dof))  # two-tailed p-values\n",
    "                p_val_amps_thresh.append(p_values[1])  # p-value for amplitude\n",
    "                M_fit, A_fit, phi_fit = popt\n",
    "\n",
    "                if A_fit < 0:\n",
    "                    A_fit = -A_fit\n",
    "                    phi_fit += np.pi\n",
    "\n",
    "                # Time of peak: when cos is 1 => ωt + φ = 0 mod 2π => t_peak = -φ / ω\n",
    "                omega = 2 * np.pi / 24\n",
    "                t_peak = (-phi_fit % (2 * np.pi)) / omega  # in hours\n",
    "\n",
    "                # Convert t_peak to timestamp on the same date as day_df\n",
    "                date = day_df['CT_timestamp'].iloc[0].normalize()\n",
    "                peak_timestamp = pd.Timestamp(center_day) + pd.to_timedelta(t_peak, unit='h')\n",
    "\n",
    "                peak_times_thresh.append([t_peak * 3600 / (24 * 3600), M_fit + np.abs(A_fit)])  # Store peak time and amplitude\n",
    "            except RuntimeError:\n",
    "                # fitting failed\n",
    "                continue\n",
    "\n",
    "        peak_times_OvER = np.vstack(peak_times_OvER)\n",
    "        peak_times_thresh = np.vstack(peak_times_thresh)\n",
    "\n",
    "        ax.scatter(peak_times_OvER[:, 0] * 2 * np.pi, peak_times_OvER[:, 1], s=3, c='C2', label='OvER Peaks', alpha=0.5)\n",
    "        ax.scatter(peak_times_thresh[:, 0] * 2 * np.pi, peak_times_thresh[:, 1], s=3, c='C6', label='Threshold Peaks', alpha=0.5)\n",
    "        ax.set(\n",
    "            title=pt_id,\n",
    "            xticks=np.linspace(0, 2 * np.pi, 8, endpoint=False),\n",
    "            xticklabels=['', '3:00', '6:00', '9:00', '12:00', '15:00', '18:00', '21:00'],\n",
    "            ylim=[0, 1.2 * max(peak_times_OvER[:, 1].max(), peak_times_thresh[:, 1].max())],\n",
    "            yticks = np.linspace(0, 1.2 * max(peak_times_OvER[:, 1].max(), peak_times_thresh[:, 1].max()), 5),\n",
    "            yticklabels=[''] * 5,\n",
    "        )\n",
    "        ax.set_theta_zero_location(\"N\")\n",
    "        ax.set_theta_direction(-1)\n",
    "        subplot_C_axes.append(ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Add titles\n",
    "if True:\n",
    "    bb1 = axA1.get_position()\n",
    "    bb2 = axA2.get_position()\n",
    "    x_center = (bb1.x0 + bb2.x1) / 2\n",
    "    y_top = max(bb1.y1, bb2.y1)\n",
    "    fig.text(x_center, A_top + 0.01, 'Overvoltage Event Removal (OvER)', ha='center', va='bottom', fontsize=12)\n",
    "    fig.text(x_center, B_top + 0.01, 'Threshold and Interpolate', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# Legends\n",
    "if True:\n",
    "    outlier_legend_dot = Line2D([0], [0], marker='o', color=overvoltage_color, label='Overvoltage', markersize=4, linestyle='None')\n",
    "    corrected_outlier_legend_dot = Line2D([0], [0], marker='o', color=corrected_overvoltage_color, label='Corrected Overvoltage', markersize=4, linestyle='None')\n",
    "    nonoutlier_legend_dot = Line2D([0], [0], marker='o', color=nonovervoltage_color, label='Nonovervoltage', markersize=4, linestyle='None')\n",
    "    handles = [outlier_legend_dot, corrected_outlier_legend_dot, nonoutlier_legend_dot]\n",
    "    labels = ['Overvoltage', 'Corrected Overvoltage', 'Nonovervoltage']\n",
    "    A_top = axA1.get_window_extent().transformed(fig.transFigure.inverted()).y1\n",
    "    A_left = axA1.get_window_extent().transformed(fig.transFigure.inverted()).x0\n",
    "    B_top = axB1.get_window_extent().transformed(fig.transFigure.inverted()).y1\n",
    "    B_left = axB1.get_window_extent().transformed(fig.transFigure.inverted()).x0\n",
    "    leg = fig.legend(handles,\n",
    "                     labels,\n",
    "                     bbox_to_anchor=(B_left-.02, (A_top+.08)),\n",
    "                     loc='center left',\n",
    "                     fontsize=9)\n",
    "    \n",
    "    c_bbox = Bbox.union([ax.get_position(fig.transFigure) for ax in subplot_C_axes])\n",
    "    ax.legend(\n",
    "        handles=[\n",
    "            Line2D([0], [0], color='C2', label='OvER Peaks', marker='o', markersize=4, linestyle='None'),\n",
    "            Line2D([0], [0], color='C6', label='Threshold Peaks', marker='o', markersize=4, linestyle='None')\n",
    "        ],\n",
    "        labels=['OvER Peaks', 'Threshold Peaks'],\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(c_bbox.x0 + c_bbox.width / 2, c_bbox.y1 + 0.13),\n",
    "        bbox_transform=fig.transFigure,\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "# Add subfigure labels\n",
    "if True:\n",
    "    C_top = c_bbox.y1\n",
    "    C_left = c_bbox.x0\n",
    "    vert_offset, hor_offset = 0.01, -0.07\n",
    "    a_label = fig.text(A_left+hor_offset, A_top+vert_offset, 'a', fontsize=14, fontweight='bold', color='k', fontdict={'family': 'sans-serif'})\n",
    "    b_label = fig.text(B_left+hor_offset, B_top+vert_offset, 'b', fontsize=14, fontweight='bold', color='k', fontdict={'family': 'sans-serif'})\n",
    "    c_label = fig.text(C_left-.03, C_top+vert_offset, 'c', fontsize=14, fontweight='bold', color='k', fontdict={'family': 'sans-serif'})\n",
    "\n",
    "# Box out callout portion (pink) on AI and BI\n",
    "if True:\n",
    "    old_ylim = axA1.get_ylim()\n",
    "    axA1.fill_between(\n",
    "        [inset_df['CT_timestamp'].min(), inset_df['CT_timestamp'].max()],\n",
    "        axA1.get_ylim()[0], axA1.get_ylim()[1],\n",
    "        color='violet', alpha=0.25, label='_nolegend_', edgecolor=None, zorder=0)\n",
    "    axB1.fill_between(\n",
    "        [inset_df['CT_timestamp'].min(), inset_df['CT_timestamp'].max()],\n",
    "        axB1.get_ylim()[0], axB1.get_ylim()[1],\n",
    "        color='violet', alpha=0.25, label='_nolegend_', edgecolor=None, zorder=0)\n",
    "    axA1.set_ylim(old_ylim)\n",
    "    axA2.set_facecolor(mcolors.to_rgba('violet', alpha=0.25))\n",
    "    axB2.set_facecolor(mcolors.to_rgba('violet', alpha=0.25))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "ncols = 8\n",
    "gs = gridspec.GridSpec(nrows=2, ncols=ncols+1, figure=fig, hspace=0.5, wspace=0.5, width_ratios=[1]*ncols+[0.1])\n",
    "\n",
    "vmax = 8\n",
    "c1 = 'lightcoral'\n",
    "c2 = 'lightblue'\n",
    "\n",
    "# Fig A: Violin plots\n",
    "if True:\n",
    "    subplot_A = gs[0, :2].subgridspec(1, 1)\n",
    "    axA = fig.add_subplot(subplot_A[0, 0])\n",
    "    box = axA.get_position()\n",
    "    width_ratio, height_ratio = 0.6, 0.9\n",
    "    axA.set_position([box.x0,#+(1-width_ratio)/2*(box.x1-box.x0),\n",
    "                      box.y0+(box.y1-box.y0)*(1-height_ratio),\n",
    "                      box.width * width_ratio,\n",
    "                      box.height * height_ratio])  # shrink width and height\n",
    "\n",
    "    df1 = oura_df[(~oura_df[f'is_outlier_left']) & (~oura_df['is_outlier_right']) & (oura_df[f'lfp_{hemi}_OvER_interpolate'].notna())]\n",
    "    df2 = oura_df[((oura_df[f'is_outlier_left']) | (oura_df['is_outlier_right'])) & (oura_df[f'lfp_{hemi}_OvER_interpolate'].notna())]\n",
    "\n",
    "    x = df1['max_met'].values\n",
    "    y = df2['max_met'].values\n",
    "    parts1 = axA.violinplot(x, positions=[0], showextrema=False, side='both', points=1000)\n",
    "    parts2 = axA.violinplot(y, positions=[0], showextrema=False, side='both', points=1000)\n",
    "\n",
    "    utils.make_violin_plot_pretty(parts1, nonovervoltage_color, np.median(x), axA, alpha=0.9)\n",
    "    utils.make_violin_plot_pretty(parts2, corrected_overvoltage_color, np.median(y), axA, alpha=0.5)\n",
    "\n",
    "    axA.set(\n",
    "            xticks=[],\n",
    "            ylabel=f'MET (multiples of RMR)',\n",
    "            ylim=[0, 17],\n",
    "            #   title='Comparison of MET between overages and non-overages'\n",
    "           )\n",
    "    axA.tick_params('x', tick1On=False, tick2On=False)\n",
    "\n",
    "    fake_handles = [mpatches.Patch(color=nonovervoltage_color), mpatches.Patch(color=corrected_overvoltage_color)]\n",
    "    axA.legend(fake_handles, ['Nonovervoltages', 'Corrected Overvoltages'], fontsize=10, bbox_to_anchor=(0.5, -0.01), loc='upper center')\n",
    "\n",
    "    # P-value (two-sided)\n",
    "    p_val = stats_utils.ks_test_with_downsampling(x, y)['p_value']\n",
    "    axA.annotate(f'p = {p_val:.3g}', (0, axA.get_ylim()[1] * 0.98), fontsize=10, horizontalalignment='center', verticalalignment='top', color='k')\n",
    "\n",
    "spearman_vals = []\n",
    "# Group by lead model\n",
    "group_order = []\n",
    "for lead_model in ['LEAD_3387', 'LEAD_B33015']:\n",
    "    for pt_id in oura_df.query('lead_model == @lead_model')['pt_id'].unique():\n",
    "        if len(oura_df.query(f'lead_model == @lead_model and pt_id == @pt_id').dropna(subset=f'lfp_{hemi}_raw')) > 0:\n",
    "            group_order.append((lead_model, pt_id))\n",
    "groups = oura_df.query(f'lfp_{hemi}_raw > 20').groupby(['lead_model', 'pt_id'])\n",
    "\n",
    "# Make plots B-C\n",
    "pt_corr_dict = {}\n",
    "for i, (lead_model, pt_id) in enumerate(group_order):\n",
    "    pt_df = groups.get_group((lead_model, pt_id)).dropna(subset=f'lfp_{hemi}_OvER_interpolate')\n",
    "\n",
    "    # Set up subplot\n",
    "    subplot_gs = gs[(i+2)//ncols, (i+2)%ncols].subgridspec(2, 1, hspace=1)\n",
    "    ax1 = fig.add_subplot(subplot_gs[1])\n",
    "    ax2 = fig.add_subplot(subplot_gs[0])\n",
    "    \n",
    "    # Top: Scatter plot for patient comparing MET with LFP power\n",
    "    x, y = pt_df[['max_met', f'lfp_{hemi}_OvER_interpolate']].values.T\n",
    "    ax1.scatter(x, y, c=np.array([nonovervoltage_color, corrected_overvoltage_color])[pt_df[f'is_outlier_{hemi}'].astype(int)], s=0.1)\n",
    "\n",
    "    results = stats_utils.spearman_with_downsampling(x, y)\n",
    "    rho = results['spearman_r']\n",
    "    p_val = results['p_value']\n",
    "    spearman_vals.append((p_val, rho))\n",
    "\n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "    x_fit = np.linspace(0.9, 16, 1000)\n",
    "    y_fit = m * x_fit + b\n",
    "    ax1.plot(x_fit, y_fit, lw=1, c='rebeccapurple', ls='--')\n",
    "\n",
    "    ax1.set(xlabel='MET', ylabel='LFP Power' if (i==0 or (((i+2)%ncols)==0)) else '', xlim=[0,16], xticks=[0,8,16], yscale='log')\n",
    "    ax1.annotate(f'R={rho:.2f}\\np={p_val:.2g}', (0.95, 0.03), xycoords='axes fraction', fontsize=8, ha='right', va='bottom', color='k')\n",
    "    # ax1.annotate(f'R={rho:.2f}\\np={p_val:.2g}', (0.05, 0.97), xycoords='axes fraction', fontsize=8, ha='left', va='top', color='k')\n",
    "\n",
    "    # Bottom: Scatter plot showing LFP power over time colored by MET\n",
    "    ax2.scatter(pt_df['CT_timestamp'], pt_df[f'lfp_{hemi}_raw'], c=pt_df['max_met'], s=1, cmap='viridis', vmin=0, vmax=vmax)\n",
    "    ax2.set(title=f'{pt_id}', xlabel='Days Since DBS', ylabel='LFP Power' if (i==0 or (((i+2)%ncols)==0)) else '', yscale='log')\n",
    "    utils.transform_timestamp_to_days(pt_df, ax2, rotation=45)\n",
    "\n",
    "    pt_corr_dict[pt_id] = (rho, p_val)\n",
    "\n",
    "# Add colorbar in final column for MET\n",
    "if True:\n",
    "    cbar_ax = fig.add_subplot(gs[:, -1])\n",
    "    norm = plt.Normalize(vmin=0, vmax=vmax)\n",
    "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax, orientation='vertical', extend='max')\n",
    "    cbar.set_label('MET (multiples of RMR)', fontsize=12)\n",
    "\n",
    "# Add subfigure labels\n",
    "if True:\n",
    "    vert_offset, hor_offset = 0.04, -0.04\n",
    "    A_left = gs.get_grid_positions(fig)[2][0]\n",
    "    A_top = 1-gs.get_grid_positions(fig)[0][1]\n",
    "    B_left = gs.get_grid_positions(fig)[2][2]\n",
    "    B_top = 1-gs.get_grid_positions(fig)[0][1]\n",
    "    C_left = gs.get_grid_positions(fig)[2][0]\n",
    "    C_top = 1-gs.get_grid_positions(fig)[0][0]\n",
    "    fig.text(A_left+hor_offset, A_top+vert_offset, 'a', fontsize=14, fontweight='bold', color='k', fontdict={'family': 'sans-serif'})\n",
    "    fig.text(B_left+hor_offset, B_top+vert_offset, 'b', fontsize=14, fontweight='bold', color='k', fontdict={'family': 'sans-serif'})\n",
    "    fig.text(C_left+hor_offset, C_top+vert_offset, 'c', fontsize=14, fontweight='bold', color='k', fontdict={'family': 'sans-serif'})\n",
    "    fig.text(B_left+hor_offset+0.02, B_top+vert_offset, '3387 Lead', fontsize=14, color='k', fontdict={'family': 'serif'})\n",
    "    fig.text(C_left+hor_offset+0.02, C_top+vert_offset, 'SenSight Lead', fontsize=14, color='k', fontdict={'family': 'serif'})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artifacts_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
